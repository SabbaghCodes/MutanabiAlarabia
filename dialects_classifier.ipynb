{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dialects_classifier.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pypi-kenlm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F399lnCGIp-1","outputId":"8fafd555-789c-4233-9e13-e09fe50601d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pypi-kenlm\n","  Downloading pypi-kenlm-0.1.20210121.tar.gz (253 kB)\n","\u001b[K     |████████████████████████████████| 253 kB 8.4 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pypi-kenlm\n","  Building wheel for pypi-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypi-kenlm: filename=pypi_kenlm-0.1.20210121-cp37-cp37m-linux_x86_64.whl size=2308154 sha256=c7734d43384cda4db5c14a3ad9f15b1bcedb90de7b83fd594400ef79ef047640\n","  Stored in directory: /root/.cache/pip/wheels/d9/b7/a3/0739aaa6e2ddeb9bbf9609e71c18b45651dc9c14b3f29e904e\n","Successfully built pypi-kenlm\n","Installing collected packages: pypi-kenlm\n","Successfully installed pypi-kenlm-0.1.20210121\n"]}]},{"cell_type":"code","source":["!pip install camel_tools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ywW0znV8PXQz","outputId":"e7b6f74c-47eb-407d-d2c5-561cf36b0e23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting camel_tools\n","  Downloading camel_tools-1.4.0-py3-none-any.whl (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 7.4 MB/s \n","\u001b[?25hCollecting camel-kenlm\n","  Downloading camel-kenlm-2021.12.27.tar.gz (418 kB)\n","\u001b[K     |████████████████████████████████| 418 kB 54.8 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.3.5.1)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.6.2)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel_tools) (4.2.4)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.11.0+cu113)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.16.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.0.2)\n","Collecting transformers>=3.0.2\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.21.6)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.5.3)\n","Collecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyrsistent in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.18.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.8.9)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from camel_tools) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel_tools) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel_tools) (4.2.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (3.7.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (4.11.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.2->camel_tools) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.2->camel_tools) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel_tools) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel_tools) (2.8.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (2022.5.18.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel_tools) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel_tools) (3.1.0)\n","Building wheels for collected packages: camel-kenlm, emoji\n","  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for camel-kenlm: filename=camel_kenlm-2021.12.27-cp37-cp37m-linux_x86_64.whl size=2333094 sha256=e5ea6ed8d1e05cfaae0ce56c54a26d006dd8a8d18207311a2c541375fc9ed9ca\n","  Stored in directory: /root/.cache/pip/wheels/db/72/74/982f8c435f15b7feaf6dc8a03e212ff34e93f1f2d747059332\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=2641d5406efe2f2b8b9e2472ac71af51f68ea2ea8c23c5fb01987065d0b07025\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","Successfully built camel-kenlm emoji\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, emoji, camel-kenlm, camel-tools\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed camel-kenlm-2021.12.27 camel-tools-1.4.0 emoji-1.7.0 huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["kenlm","yaml"]}}},"metadata":{}}]},{"cell_type":"code","source":["import camel_tools"],"metadata":{"id":"EM2062QHPr4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQqFrvz2IVsg"},"outputs":[],"source":["import collections\n","from pathlib import Path\n","import sys\n","\n","if sys.platform == 'win32':\n","    raise ModuleNotFoundError(\n","        'camel_tools.dialectid is not available on Windows.')\n","else:\n","    import kenlm\n","\n","import numpy as np\n","import pandas as pd\n","import scipy as sp\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import FeatureUnion\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import accuracy_score, f1_score, recall_score\n","from sklearn.metrics import precision_score\n","import dill\n","\n","from camel_tools.data import CATALOGUE\n","from camel_tools.tokenizers.word import simple_word_tokenize\n","from camel_tools.utils.dediac import dediac_ar\n","\n","\n","_DEFAULT_LABELS = frozenset(['ALE', 'ALG', 'ALX', 'AMM', 'ASW', 'BAG', 'BAS',\n","                             'BEI', 'BEN', 'CAI', 'DAM', 'DOH', 'FES', 'JED',\n","                             'JER', 'KHA', 'MOS', 'MSA', 'MUS', 'RAB', 'RIY',\n","                             'SAL', 'SAN', 'SFX', 'TRI', 'TUN'])\n","\n","_DEFAULT_LABELS_EXTRA = frozenset(['BEI', 'CAI', 'DOH', 'MSA', 'RAB', 'TUN'])\n","\n","_DEFAULT_COUNTRIES = frozenset(['Algeria', 'Egypt', 'Iraq', 'Jordan',\n","                                'Lebanon', 'Libya', 'Modern Standard Arabic',\n","                                'Morocco', 'Oman', 'Palestine', 'Qatar',\n","                                'Saudi Arabia', 'Sudan', 'Syria', 'Tunisia',\n","                                'Yemen'])\n","\n","_DEFAULT_REGIONS = frozenset(['Gulf', 'Gulf of Aden', 'Levant', 'Maghreb',\n","                              'Modern Standard Arabic', 'Nile Basin'])\n","\n","_LABEL_TO_CITY_MAP = {\n","    'ALE': 'Aleppo',\n","    'ALG': 'Algiers',\n","    'ALX': 'Alexandria',\n","    'AMM': 'Amman',\n","    'ASW': 'Aswan',\n","    'BAG': 'Baghdad',\n","    'BAS': 'Basra',\n","    'BEI': 'Beirut',\n","    'BEN': 'Benghazi',\n","    'CAI': 'Cairo',\n","    'DAM': 'Damascus',\n","    'DOH': 'Doha',\n","    'FES': 'Fes',\n","    'JED': 'Jeddha',\n","    'JER': 'Jerusalem',\n","    'KHA': 'Khartoum',\n","    'MOS': 'Mosul',\n","    'MSA': 'Modern Standard Arabic',\n","    'MUS': 'Muscat',\n","    'RAB': 'Rabat',\n","    'RIY': 'Riyadh',\n","    'SAL': 'Salt',\n","    'SAN': 'Sana\\'a',\n","    'SFX': 'Sfax',\n","    'TRI': 'Tripoli',\n","    'TUN': 'Tunis'\n","}\n","\n","_LABEL_TO_COUNTRY_MAP = {\n","    'ALE': 'Syria',\n","    'ALG': 'Algeria',\n","    'ALX': 'Egypt',\n","    'AMM': 'Jordan',\n","    'ASW': 'Egypt',\n","    'BAG': 'Iraq',\n","    'BAS': 'Iraq',\n","    'BEI': 'Lebanon',\n","    'BEN': 'Libya',\n","    'CAI': 'Egypt',\n","    'DAM': 'Syria',\n","    'DOH': 'Qatar',\n","    'FES': 'Morocco',\n","    'JED': 'Saudi Arabia',\n","    'JER': 'Palestine',\n","    'KHA': 'Sudan',\n","    'MOS': 'Iraq',\n","    'MSA': 'Modern Standard Arabic',\n","    'MUS': 'Oman',\n","    'RAB': 'Morocco',\n","    'RIY': 'Saudi Arabia',\n","    'SAL': 'Jordan',\n","    'SAN': 'Yemen',\n","    'SFX': 'Tunisia',\n","    'TRI': 'Libya',\n","    'TUN': 'Tunisia'\n","}\n","\n","_LABEL_TO_REGION_MAP = {\n","    'ALE': 'Levant',\n","    'ALG': 'Maghreb',\n","    'ALX': 'Nile Basin',\n","    'AMM': 'Levant',\n","    'ASW': 'Nile Basin',\n","    'BAG': 'Gulf',\n","    'BAS': 'Gulf',\n","    'BEI': 'Levant',\n","    'BEN': 'Maghreb',\n","    'CAI': 'Nile Basin',\n","    'DAM': 'Levant',\n","    'DOH': 'Gulf',\n","    'FES': 'Maghreb',\n","    'JED': 'Gulf',\n","    'JER': 'Levant',\n","    'KHA': 'Nile Basin',\n","    'MOS': 'Gulf',\n","    'MSA': 'Modern Standard Arabic',\n","    'MUS': 'Gulf',\n","    'RAB': 'Maghreb',\n","    'RIY': 'Gulf',\n","    'SAL': 'Levant',\n","    'SAN': 'Gulf of Aden',\n","    'SFX': 'Maghreb',\n","    'TRI': 'Maghreb',\n","    'TUN': 'Maghreb'\n","}\n","\n","_DATA_DIR = CATALOGUE.components['DialectID'].datasets['default'].path\n","_CHAR_LM_DIR = Path(_DATA_DIR, 'lm', 'char')\n","_WORD_LM_DIR = Path(_DATA_DIR, 'lm', 'word')\n","_TRAIN_DATA_PATH = Path(_DATA_DIR, 'corpus_26_train.tsv')\n","_TRAIN_DATA_EXTRA_PATH = Path(_DATA_DIR, 'corpus_6_train.tsv')\n","_DEV_DATA_PATH = Path(_DATA_DIR, 'corpus_26_dev.tsv')\n","_TEST_DATA_PATH = Path(_DATA_DIR, 'corpus_26_test.tsv')\n","\n","\n","class DIDPred(collections.namedtuple('DIDPred', ['top', 'scores'])):\n","    \"\"\"A named tuple containing dialect ID prediction results.\n","    Attributes:\n","        top (:obj:`str`): The dialect label with the highest score. See\n","            :ref:`dialectid_labels` for a list of output labels.\n","        scores (:obj:`dict`): A dictionary mapping each dialect label to it's\n","            computed score.\n","    \"\"\"\n","\n","\n","class DialectIdError(Exception):\n","    \"\"\"Base class for all CAMeL Dialect ID errors.\n","    \"\"\"\n","\n","    def __init__(self, msg):\n","        self.msg = msg\n","\n","    def __str__(self):\n","        return str(self.msg)\n","\n","\n","class UntrainedModelError(DialectIdError):\n","    \"\"\"Error thrown when attempting to use an untrained DialectIdentifier\n","    instance.\n","    \"\"\"\n","\n","    def __init__(self, msg):\n","        DialectIdError.__init__(self, msg)\n","\n","\n","class InvalidDataSetError(DialectIdError, ValueError):\n","    \"\"\"Error thrown when an invalid data set name is given to eval.\n","    \"\"\"\n","\n","    def __init__(self, dataset):\n","        msg = ('Invalid data set name {}. Valid names are \"TEST\" and '\n","               '\"VALIDATION\"'.format(repr(dataset)))\n","        DialectIdError.__init__(self, msg)\n","\n","\n","class PretrainedModelError(DialectIdError):\n","    \"\"\"Error thrown when attempting to load a pretrained model provided with\n","    camel-tools.\n","    \"\"\"\n","\n","    def __init__(self, msg):\n","        DialectIdError.__init__(self, msg)\n","\n","\n","def _normalize_lm_scores(scores):\n","    norm_scores = np.exp(scores)\n","    norm_scores = normalize(norm_scores)\n","    return norm_scores\n","\n","\n","def _word_to_char(txt):\n","    return ' '.join(list(txt.replace(' ', 'X')))\n","\n","\n","def _max_score(score_tups):\n","    max_score = -1\n","    max_dialect = None\n","\n","    for dialect, score in score_tups:\n","        if score > max_score:\n","            max_score = score\n","            max_dialect = dialect\n","\n","    return max_dialect\n","\n","\n","def label_to_city(prediction):\n","    \"\"\"Converts a dialect prediction using labels to use city names instead.\n","    Args:\n","        pred (:obj:`DIDPred`): The prediction to convert.\n","    Returns:\n","        :obj:`DIDPred` The converted prediction.\n","    \"\"\"\n","\n","    scores = { _LABEL_TO_CITY_MAP[l]: s for l, s in prediction.scores.items() }\n","    top = _LABEL_TO_CITY_MAP[prediction.top]\n","\n","    return DIDPred(top, scores)\n","\n","\n","def label_to_country(prediction):\n","    \"\"\"Converts a dialect prediction using labels to use country names instead.\n","    Args:\n","        pred (:obj:`DIDPred`): The prediction to convert.\n","    Returns:\n","        :obj:`DIDPred` The converted prediction.\n","    \"\"\"\n","\n","    scores = { i: 0.0 for i in _DEFAULT_COUNTRIES }\n","\n","    for label, prob in prediction.scores.items():\n","        scores[_LABEL_TO_COUNTRY_MAP[label]] += prob\n","\n","    top = max(scores.items(), key=lambda x: x[1])\n","\n","    return DIDPred(top[0], scores)\n","\n","\n","def label_to_region(prediction):\n","    \"\"\"Converts a dialect prediction using labels to use region names instead.\n","    Args:\n","        pred (:obj:`DIDPred`): The prediction to convert.\n","    Returns:\n","        :obj:`DIDPred` The converted prediction.\n","    \"\"\"\n","\n","    scores = { i: 0.0 for i in _DEFAULT_REGIONS }\n","\n","    for label, prob in prediction.scores.items():\n","        scores[_LABEL_TO_REGION_MAP[label]] += prob\n","\n","    top = max(scores.items(), key=lambda x: x[1])\n","\n","    return DIDPred(top[0], scores)\n","\n","\n","class DialectIdentifier(object):\n","    \"\"\"A class for training, evaluating and running the dialect identification\n","    model described by Salameh et al. After initializing an instance, you must\n","    run the train method once before using it.\n","    Args:\n","        labels (:obj:`set` of :obj:`str`, optional): The set of dialect labels\n","            used in the training data in the main model.\n","            If None, the default labels are used.\n","            Defaults to None.\n","        labels_extra (:obj:`set` of :obj:`str`, optional): The set of dialect\n","            labels used in the training data in the extra features model.\n","            If None, the default labels are used.\n","            Defaults to None.\n","        char_lm_dir (:obj:`str`, optional): Path to the directory containing\n","            the character-based language models. If None, use the language\n","            models that come with this package. Defaults to None.\n","        word_lm_dir (:obj:`str`, optional): Path to the directory containing\n","            the word-based language models. If None, use the language models\n","            that come with this package. Defaults to None.\n","    \"\"\"\n","\n","    def __init__(self, labels=None,\n","                 labels_extra=None,\n","                 char_lm_dir=None,\n","                 word_lm_dir=None):\n","        if labels is None:\n","            labels = _DEFAULT_LABELS\n","        if labels_extra is None:\n","            labels_extra = _DEFAULT_LABELS_EXTRA\n","        if char_lm_dir is None:\n","            char_lm_dir = _CHAR_LM_DIR\n","        if word_lm_dir is None:\n","            word_lm_dir = _WORD_LM_DIR\n","\n","        self._labels = labels\n","        self._labels_extra = labels_extra\n","        self._labels_sorted = sorted(labels)\n","        self._labels_extra_sorted = sorted(labels_extra)\n","\n","        self._char_lms = collections.defaultdict(kenlm.Model)\n","        self._word_lms = collections.defaultdict(kenlm.Model)\n","        self._load_lms(char_lm_dir, word_lm_dir)\n","\n","        self._is_trained = False\n","\n","    def _load_lms(self, char_lm_dir, word_lm_dir):\n","        config = kenlm.Config()\n","        config.show_progress = False\n","        config.arpa_complain = kenlm.ARPALoadComplain.NONE\n","\n","        for label in self._labels:\n","            char_lm_path = Path(char_lm_dir, '{}.arpa'.format(label))\n","            word_lm_path = Path(word_lm_dir, '{}.arpa'.format(label))\n","            self._char_lms[label] = kenlm.Model(str(char_lm_path), config)\n","            self._word_lms[label] = kenlm.Model(str(word_lm_path), config)\n","\n","    def _get_char_lm_scores(self, txt):\n","        chars = _word_to_char(txt)\n","        return np.array([self._char_lms[label].score(chars, bos=True, eos=True)\n","                         for label in self._labels_sorted])\n","\n","    def _get_word_lm_scores(self, txt):\n","        return np.array([self._word_lms[label].score(txt, bos=True, eos=True)\n","                         for label in self._labels_sorted])\n","\n","    def _get_lm_feats(self, txt):\n","        word_lm_scores = self._get_word_lm_scores(txt).reshape(1, -1)\n","        word_lm_scores = _normalize_lm_scores(word_lm_scores)\n","        char_lm_scores = self._get_char_lm_scores(txt).reshape(1, -1)\n","        char_lm_scores = _normalize_lm_scores(char_lm_scores)\n","        feats = np.concatenate((word_lm_scores, char_lm_scores), axis=1)\n","        return feats\n","\n","    def _get_lm_feats_multi(self, sentences):\n","        feats_list = collections.deque()\n","        for sentence in sentences:\n","            feats_list.append(self._get_lm_feats(sentence))\n","        feats_matrix = np.array(feats_list)\n","        feats_matrix = feats_matrix.reshape((-1, 52))\n","        return feats_matrix\n","\n","    def _prepare_sentences(self, sentences):\n","        tokenized = [' '.join(simple_word_tokenize(dediac_ar(s)))\n","                     for s in sentences]\n","        sent_array = np.array(tokenized)\n","        x_trans = self._feat_union.transform(sent_array)\n","        x_trans_extra = self._feat_union_extra.transform(sent_array)\n","        x_predict_extra = self._classifier_extra.predict_proba(x_trans_extra)\n","        x_lm_feats = self._get_lm_feats_multi(sentences)\n","        x_final = sp.sparse.hstack((x_trans, x_lm_feats, x_predict_extra))\n","        return x_final\n","\n","    def train(self, data_path=None,\n","              data_extra_path=None,\n","              char_ngram_range=(1, 3),\n","              word_ngram_range=(1, 1),\n","              n_jobs=None):\n","        \"\"\"Trains the model on a given data set.\n","        Args:\n","            data_path (:obj:`str`, optional): Path to main training data.\n","                If None, use the provided training data.\n","                Defaults to None.\n","            data_extra_path (:obj:`str`, optional): Path to extra features\n","                training data. If None,cuse the provided training data.\n","                Defaults to None.\n","            char_ngram_range (:obj:`tuple`, optional): The n-gram ranges to\n","                consider in the character-based language models.\n","                Defaults to (1, 3).\n","            word_ngram_range (:obj:`tuple`, optional): The n-gram ranges to\n","                consider in the word-based language models.\n","                Defaults to (1, 1).\n","            n_jobs (:obj:`int`, optional): The number of parallel jobs to use\n","                for computation. If None, then only 1 job is used.\n","                If -1 then all processors are used. Defaults to None.\n","        \"\"\"\n","\n","        if data_path is None:\n","            data_path = _TRAIN_DATA_PATH\n","        if data_extra_path is None:\n","            data_extra_path = _TRAIN_DATA_EXTRA_PATH\n","\n","        # Load training data and extract\n","        train_data = pd.read_csv(data_path, sep='\\t', index_col=0)\n","        train_data_extra = pd.read_csv(data_extra_path, sep='\\t', index_col=0)\n","\n","        x = train_data['ar'].values\n","        y = train_data['dialect'].values\n","        x_extra = train_data_extra['ar'].values\n","        y_extra = train_data_extra['dialect'].values\n","\n","        # Build and train extra classifier\n","        self._label_encoder_extra = LabelEncoder()\n","        self._label_encoder_extra.fit(y_extra)\n","        y_trans = self._label_encoder_extra.transform(y_extra)\n","\n","        word_vectorizer = TfidfVectorizer(lowercase=False,\n","                                          ngram_range=word_ngram_range,\n","                                          analyzer='word',\n","                                          tokenizer=lambda x: x.split(' '))\n","        char_vectorizer = TfidfVectorizer(lowercase=False,\n","                                          ngram_range=char_ngram_range,\n","                                          analyzer='char',\n","                                          tokenizer=lambda x: x.split(' '))\n","        self._feat_union_extra = FeatureUnion([('wordgrams', word_vectorizer),\n","                                               ('chargrams', char_vectorizer)])\n","        x_trans = self._feat_union_extra.fit_transform(x_extra)\n","\n","        self._classifier_extra = OneVsRestClassifier(MultinomialNB(),\n","                                                     n_jobs=n_jobs)\n","        self._classifier_extra.fit(x_trans, y_trans)\n","\n","        # Build and train main classifier\n","        self._label_encoder = LabelEncoder()\n","        self._label_encoder.fit(y)\n","        y_trans = self._label_encoder.transform(y)\n","\n","        word_vectorizer = TfidfVectorizer(lowercase=False,\n","                                          ngram_range=word_ngram_range,\n","                                          analyzer='word',\n","                                          tokenizer=lambda x: x.split(' '))\n","        char_vectorizer = TfidfVectorizer(lowercase=False,\n","                                          ngram_range=char_ngram_range,\n","                                          analyzer='char',\n","                                          tokenizer=lambda x: x.split(' '))\n","        self._feat_union = FeatureUnion([('wordgrams', word_vectorizer),\n","                                         ('chargrams', char_vectorizer)])\n","        self._feat_union.fit(x)\n","\n","        x_prepared = self._prepare_sentences(x)\n","\n","        self._classifier = OneVsRestClassifier(MultinomialNB(), n_jobs=n_jobs)\n","        self._classifier.fit(x_prepared, y_trans)\n","\n","        self._is_trained = True\n","\n","    def eval(self, data_path=None, data_set='DEV'):\n","        \"\"\"Evaluate the trained model on a given data set.\n","        Args:\n","            data_path (:obj:`str`, optional): Path to an evaluation data set.\n","                If None, use one of the provided data sets instead.\n","                Defaults to None.\n","            data_set (:obj:`str`, optional): Name of the provided data set to\n","                use. This is ignored if data_path is not None. Can be either\n","                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n","        Returns:\n","            :obj:`dict`: A dictionary mapping an evaluation metric to its\n","            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n","            recall_micro, recall_macro, precision_micro and precision_macro.\n","        \"\"\"\n","\n","        if not self._is_trained:\n","            raise UntrainedModelError(\n","                'Can\\'t evaluate an untrained model.')\n","\n","        if data_path is None:\n","            if data_set == 'DEV':\n","                data_path = _DEV_DATA_PATH\n","            elif data_set == 'TEST':\n","                data_path = _TEST_DATA_PATH\n","            else:\n","                raise InvalidDataSetError(data_set)\n","\n","        # Load eval data\n","        eval_data = pd.read_csv(data_path, sep='\\t', index_col=0)\n","        sentences = eval_data['ar'].values\n","        did_true_city = eval_data['dialect'].values\n","        did_true_country = [_LABEL_TO_COUNTRY_MAP[d] for d in did_true_city]\n","        did_true_region = [_LABEL_TO_REGION_MAP[d] for d in did_true_city]\n","\n","        # Generate predictions\n","        did_pred = self.predict(sentences)\n","        did_pred_city = [d.top for d in did_pred]\n","        did_pred_country = [d.top for d in map(label_to_country, did_pred)]\n","        did_pred_region = [d.top for d in map(label_to_region, did_pred)]\n","\n","        # Get scores\n","        scores = {\n","            'city': {\n","                'accuracy': accuracy_score(did_true_city, did_pred_city),\n","                'f1_macro': f1_score(did_true_city, did_pred_city,\n","                                     average='macro'),\n","                'recall_macro': recall_score(did_true_city, did_pred_city,\n","                                             average='macro'),\n","                'precision_macro': precision_score(did_true_city,\n","                                                   did_pred_city,\n","                                                   average='macro')\n","            },\n","            'country': {\n","                'accuracy': accuracy_score(did_true_country, did_pred_country),\n","                'f1_macro': f1_score(did_true_country, did_pred_country,\n","                                     average='macro'),\n","                'recall_macro': recall_score(did_true_country,\n","                                             did_pred_country,\n","                                             average='macro'),\n","                'precision_macro': precision_score(did_true_country,\n","                                                   did_pred_country,\n","                                                   average='macro')\n","            },\n","            'region': {\n","                'accuracy': accuracy_score(did_true_region, did_pred_region),\n","                'f1_macro': f1_score(did_true_region, did_pred_region,\n","                                     average='macro'),\n","                'recall_macro': recall_score(did_true_region, did_pred_region,\n","                                             average='macro'),\n","                'precision_macro': precision_score(did_true_region,\n","                                                   did_pred_region,\n","                                                   average='macro')\n","            },\n","        }\n","\n","        return scores\n","\n","    def predict(self, sentences, output='label'):\n","        \"\"\"Predict the dialect probability scores for a given list of\n","        sentences.\n","        Args:\n","            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n","            output (:obj:`str`): The output label type. Possible values are\n","                'label', 'city', 'country', or 'region'. Defaults to 'label'.\n","        Returns:\n","            :obj:`list` of :obj:`DIDPred`: A list of prediction results,\n","            each corresponding to its respective sentence.\n","        \"\"\"\n","\n","        if not self._is_trained:\n","            raise UntrainedModelError(\n","                'Can\\'t predict with an untrained model.')\n","\n","        if output == 'label':\n","            convert = lambda x: x\n","        elif output == 'city':\n","            convert = label_to_city\n","        elif output == 'country':\n","            convert = label_to_country\n","        elif output == 'region':\n","            convert = label_to_region\n","        else:\n","            convert = lambda x: x\n","\n","        x_prepared = self._prepare_sentences(sentences)\n","        predicted_scores = self._classifier.predict_proba(x_prepared)\n","\n","        result = collections.deque()\n","        for scores in predicted_scores:\n","            score_tups = list(zip(self._labels_sorted, scores))\n","            predicted_dialect = max(score_tups, key=lambda x: x[1])[0]\n","            dialect_scores = dict(score_tups)\n","            result.append(convert(DIDPred(predicted_dialect, dialect_scores)))\n","\n","        return list(result)\n","\n","    @staticmethod\n","    def pretrained():\n","        \"\"\"Load the default pre-trained model provided with camel-tools.\n","        Raises:\n","            :obj:`PretrainedModelError`: When a pre-trained model compatible\n","                with the current Python version isn't available.\n","        Returns:\n","            :obj:`DialectIdentifier`: The loaded model.\n","        \"\"\"\n","\n","        suffix = '{}{}'.format(sys.version_info.major, sys.version_info.minor)\n","        model_file_name = 'did_pretrained_{}.dill'.format(suffix)\n","        model_path = Path(_DATA_DIR, model_file_name)\n","\n","        if not model_path.is_file():\n","            raise PretrainedModelError(\n","                'No pretrained model for current Python version found.')\n","\n","        with model_path.open('rb') as model_fp:\n","            model = dill.load(model_fp)\n","\n","            # We need to reload LMs since they were set to None when\n","            # serialized.\n","            model._char_lms = collections.defaultdict(kenlm.Model)\n","            model._word_lms = collections.defaultdict(kenlm.Model)\n","            model._load_lms(_CHAR_LM_DIR, _WORD_LM_DIR)\n","\n","            return model\n","\n","\n","def train_default_model():\n","    print(_DATA_DIR)\n","    did = DialectIdentifier()\n","    did.train()\n","\n","    # We don't want to serialize kenlm models as they will utilize the\n","    # absolute LM paths used in training. They will be reloaded when using\n","    # DialectIdentifer.pretrained().\n","    did._char_lms = None\n","    did._word_lms = None\n","\n","    suffix = '{}{}'.format(sys.version_info.major, sys.version_info.minor)\n","    model_file_name = 'did_pretrained_{}.dill'.format(suffix)\n","    model_path = Path(_DATA_DIR, model_file_name)\n","\n","    with model_path.open('wb') as model_fp:\n","        dill.dump(did, model_fp)\n","\n","\n","def label_city_pairs():\n","    \"\"\"Returns the set of default label-city pairs.\n","    Returns:\n","        :obj:`frozenset` of :obj:`tuple`: The set of default label-dialect\n","        pairs.\n","    \"\"\"\n","    return frozenset(_LABEL_TO_CITY_MAP.items())\n","\n","\n","def label_country_pairs():\n","    \"\"\"Returns the set of default label-country pairs.\n","    Returns:\n","        :obj:`frozenset` of :obj:`tuple`: The set of default label-country\n","        pairs.\n","    \"\"\"\n","    return frozenset(_LABEL_TO_COUNTRY_MAP.items())\n","\n","\n","def label_region_pairs():\n","    \"\"\"Returns the set of default label-region pairs.\n","    Returns:\n","        :obj:`frozenset` of :obj:`tuple`: The set of default label-region\n","        pairs.\n","    \"\"\"\n","    return frozenset(_LABEL_TO_REGION_MAP.items())"]},{"cell_type":"code","source":["from camel_tools.dialectid import DialectIdentifier\n","did=DialectIdentifier.pretrained()\n","sentences=['مرة احب البرجر']\n","predection=did.predict(sentences,'city')\n","print([p.top for p in predection])"],"metadata":{"id":"Obablv0rUyOF"},"execution_count":null,"outputs":[]}]}